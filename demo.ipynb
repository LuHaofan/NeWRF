{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a06e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from pipelines import pipeline\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from tqdm import trange\n",
    "from loaders import DatasetLoader\n",
    "import os, random, string\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffec3689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardware\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8a2781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(n_iters, cfg, dataset_fname, file_dir, ckpt = None):\n",
    "    # Set seed for reproducability\n",
    "    ###################### Load Datasets #######################\n",
    "    logging.info(\"Loading dataset....\")\n",
    "    dsLoader = DatasetLoader(dataset_fname)\n",
    "    dsLoader.split_train_val(ratio = 0.8)\n",
    "    ############################################################\n",
    "    \n",
    "    # Initialize models\n",
    "    (model, \n",
    "     fine_model, \n",
    "     encode, \n",
    "     encode_viewdirs,\n",
    "     optimizer, \n",
    "     scheduler,\n",
    "     synthesizer,\n",
    "     _\n",
    "    ) = init_models(cfg, device, ckpt)\n",
    "\n",
    "    # Loss function\n",
    "    loss_fn = lambda pred, true: torch.sum(torch.abs(pred-true)**2)/torch.sum(torch.abs(true)**2) \n",
    "    # Get the list of all frequency channels\n",
    "    fc_lst = dsLoader.get_channel_freqs(cfg.training.n_chs)\n",
    "    \n",
    "    # Sampling configurations\n",
    "    kwargs_sample_stratified = {\n",
    "        'n_samples': cfg.sampling.n_samples,\n",
    "        'perturb': cfg.sampling.perturb,\n",
    "        'inverse_depth': cfg.sampling.inverse_depth\n",
    "    }\n",
    "    \n",
    "    kwargs_sample_hierarchical = {\n",
    "        \"perturb\": cfg.sampling.perturb_hierarchical,\n",
    "        \"n_new_samples\": cfg.sampling.n_samples_hierarchical\n",
    "    }\n",
    "    \n",
    "    # Training iterations\n",
    "    train_psnrs = []\n",
    "    val_coarse_psnrs = []\n",
    "    val_fine_psnrs = []\n",
    "    iternum = []\n",
    "    for i in trange(n_iters):\n",
    "        logging.debug(f\"Iteration: {i}\")\n",
    "        # Run the training pipeline\n",
    "        try: \n",
    "            sta_id = np.random.choice(dsLoader.trainset, cfg.training.batch_size)\n",
    "            logging.info(f\"sample batch: {sta_id}\")\n",
    "            (\n",
    "                total_loss_coarse, \n",
    "                total_loss_fine,\n",
    "                _, \n",
    "                _, \n",
    "                _, \n",
    "                _, \n",
    "            ) = pipeline(cfg,\n",
    "                        sta_id,\n",
    "                        dsLoader,\n",
    "                        model, \n",
    "                        fine_model, \n",
    "                        encode, \n",
    "                        encode_viewdirs, \n",
    "                        optimizer,\n",
    "                        fc_lst,\n",
    "                        loss_fn,\n",
    "                        synthesizer,\n",
    "                        kwargs_sample_stratified,\n",
    "                        kwargs_sample_hierarchical,\n",
    "                        device,\n",
    "                        mode = 'Train')\n",
    "\n",
    "            train_psnrs.append(-10. * np.log10(total_loss_fine))\n",
    "    #         Save a checkpoint at given rate\n",
    "            if i % cfg.training.save_rate == 0 or i == n_iters-1:\n",
    "                logging.info(\"Saving a checkpoint...\")\n",
    "                with torch.no_grad():\n",
    "                    save_ckpt(i, model.state_dict(), fine_model.state_dict(), optimizer.state_dict(), file_dir)\n",
    "\n",
    "            # Evaluate at given display rate.\n",
    "            if i % cfg.training.display_rate == 0:\n",
    "                with torch.no_grad():\n",
    "                    sta_id = dsLoader.valset\n",
    "                    (\n",
    "                        total_loss_coarse, \n",
    "                        total_loss_fine,\n",
    "                        cfr_pred_coarse, \n",
    "                        cfr_pred_fine, \n",
    "                        total_weights_coarse, \n",
    "                        total_weights_fine\n",
    "                    ) = pipeline(cfg,\n",
    "                                sta_id,\n",
    "                                dsLoader,\n",
    "                                model, \n",
    "                                fine_model, \n",
    "                                encode, \n",
    "                                encode_viewdirs, \n",
    "                                optimizer,\n",
    "                                fc_lst,\n",
    "                                loss_fn,\n",
    "                                synthesizer,\n",
    "                                kwargs_sample_stratified,\n",
    "                                kwargs_sample_hierarchical,\n",
    "                                device,\n",
    "                                mode = 'Eval')\n",
    "                    val_coarse_psnrs.append(-10 * np.log10(total_loss_coarse))\n",
    "                    val_fine_psnrs.append(-10. * np.log10(total_loss_fine))\n",
    "                    iternum.append(i)\n",
    "                    if scheduler is not None:\n",
    "                        scheduler.step(total_loss_fine+total_loss_coarse)\n",
    "                    #-------------- Plot results-------------------------------\n",
    "                    fig, ax = plt.subplots(1, 3, figsize=(12,4))\n",
    "                    target_cfr = np.concatenate(dsLoader.get_cfr_batch(1, sta_id, fc_lst), axis=0).flatten()\n",
    "\n",
    "                    ax[0].plot(np.real(target_cfr), np.imag(target_cfr), \"ro\", label = \"GT\")\n",
    "                    ax[0].plot(np.real(cfr_pred_coarse.cpu()), np.imag(cfr_pred_coarse.cpu()), \"bo\", label = \"Coarse\")\n",
    "                    ax[0].legend()\n",
    "                    \n",
    "                    ax[1].plot(np.real(target_cfr), np.imag(target_cfr), \"ro\", label = \"GT\")\n",
    "                    ax[1].plot(np.real(cfr_pred_fine.cpu()), np.imag(cfr_pred_fine.cpu()), \"bo\", label = \"Fine\")\n",
    "                    ax[1].legend()\n",
    "                    \n",
    "                    ax[2].plot(range(0, i + 1), train_psnrs, 'r', label = \"Train Fine\")\n",
    "                    ax[2].plot(iternum, val_coarse_psnrs, 'y', label = 'Val Coarse')\n",
    "                    ax[2].plot(iternum, val_fine_psnrs, 'b', label = 'Val Fine')\n",
    "                    ax[2].legend()\n",
    "                    plt.show()\n",
    "                    del cfr_pred_coarse, cfr_pred_fine, total_weights_coarse, total_weights_fine, total_loss_coarse, total_loss_fine\n",
    "\n",
    "        except Exception: \n",
    "            save_ckpt(i, model.state_dict(), fine_model.state_dict(), optimizer.state_dict(), file_dir)  \n",
    "            raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3149e919",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######################## Configurations ###################\n",
    "cfg = OmegaConf.load('./config/default.yaml')\n",
    "ofdm_cfg = OmegaConf.load('./config/ofdm.yaml')\n",
    "cfg = OmegaConf.merge(cfg, ofdm_cfg)\n",
    "###########################################################\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "datadir = \"./data/\"\n",
    "dataset_fname = os.path.join(datadir, \"dataset_conference_ch1_rt_image_fc.pkl\")\n",
    "try:\n",
    "    file_dir = os.path.join(cfg.training.save_dir, ''.join(random.choices(string.ascii_uppercase + string.digits, k=5)))\n",
    "    os.mkdir(file_dir)\n",
    "except:\n",
    "    file_dir = None\n",
    "# Speficify the checkpoint file name to load if retraining\n",
    "#ckpt_fname = \"./ckpt/3405-bird-restful/ckpt_iter_9999.pt\"\n",
    "ckpt_fname = None\n",
    "main(cfg.training.n_iters, cfg, dataset_fname = dataset_fname, file_dir = file_dir, ckpt=ckpt_fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
